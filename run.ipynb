{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook to run the ML Road Segmentation Project by the lla_team in a Google Colab environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import requirement.txt file to colab then run the following cell to install the correct requirements. Then restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import ImageFile, ImageEnhance, ImageFilter\n",
    "import sys, getopt\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "sys.path.append('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount your drive to access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Parameters of the run here it is particularly important to change the submission directory and path and the data path to something on your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 19\n",
    "DATA_PATH = \"./drive/MyDrive/EPFL/data\"\n",
    "SUBMISSION_DIR = \"./drive/MyDrive/EPFL/submission/\"\n",
    "SUBMISSION_PATH = \"./drive/MyDrive/EPFL/submission/submission.csv\"\n",
    "PATCH_SIZE = 80\n",
    "BATCH_SIZE = 10\n",
    "LR = 0.001\n",
    "MAX_ITER = 64\n",
    "TH = 0.25\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEST_SIZE = 0.25\n",
    "K_FOLD = 5\n",
    "\n",
    "THRESHOLD_VALIDATION_VECTOR = [0.20, 0.25, 0.30, 0.35] # best found 0.25\n",
    "LEARNING_RATE_VALIDATION_VECTOR = [0.0003, 0.0001, 0.001, 0.01]  # best found 0.0003"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some of the modes to run in, if you want to run cross-validation set the mode to True. Specific experiments can also be chosen further down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False\n",
    "divide_patches = True\n",
    "augment = True\n",
    "training = True\n",
    "submmission = True\n",
    "validation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a pre-trained neural network to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def deeplab_model(encoder = 'resnet34'):\n",
    "    return smp.DeepLabV3(encoder_name=encoder, encoder_depth=5, encoder_weights=\"imagenet\",in_channels=3,classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image:np.ndarray, device = None, divide = False):\n",
    "    image = np.transpose(image,[2,0,1])\n",
    "    tensor = torch.Tensor(image)\n",
    "    if divide:\n",
    "        tensor = tensor / 255\n",
    "    tensor.unsqueeze(0)\n",
    "    if device is not None:\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "def mask_to_tensor(mask: np.ndarray, device = None):\n",
    "    tensor = transforms.ToTensor()(mask)\n",
    "    tensor = torch.round(tensor)\n",
    "    if device is not None:\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor[0,:,:][None,:,:]\n",
    "\n",
    "def transform_to_patch(pixels, th):\n",
    "    m = np.mean(pixels)\n",
    "    if m  > th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def transform_prediction_to_patch(img, id, patch_size=16,step=16,th=0.25):\n",
    "    prs = []\n",
    "    ids = []\n",
    "    for j in range(0,img.shape[1],step):\n",
    "        for i in range(0, img.shape[0],step):\n",
    "            prs.append(transform_to_patch(img[i:i+patch_size,j:j+patch_size],th=th))\n",
    "            ids.append(\"{:03d}_{}_{}\".format(id, j, i))\n",
    "    return prs, ids\n",
    "\n",
    "def images_to_np_array(images):\n",
    "    res = []\n",
    "    for img in images:\n",
    "        img = np.array(img)\n",
    "        res.append(img)\n",
    "    return np.array(res)\n",
    "\n",
    "def PIL_Images_from_np_array(images):\n",
    "    res = []\n",
    "    for img in images:\n",
    "        res.append(Image.fromarray(img))\n",
    "    return res\n",
    "\n",
    "def split_data(x,y):\n",
    "    np.random.seed(SEED)\n",
    "    ids = np.arange(len(x))\n",
    "\n",
    "    np.random.shuffle(ids)\n",
    "\n",
    "    division = len(ids)*(1-TEST_SIZE)\n",
    "    division = int(division)\n",
    "    return np.array(x)[ids[:division]], np.array(y)[ids[:division]], np.array(x)[ids[division:]], np.array(y)[ids[division:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing and postprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, gts, operations, train: bool):\n",
    "    data = images_to_np_array(data)\n",
    "    if train:\n",
    "        gts = images_to_np_array(gts)\n",
    "    else:\n",
    "        gts = None\n",
    "    \n",
    "    if operations['normalization']:\n",
    "        for i, image in enumerate(data):\n",
    "            image = image / 255\n",
    "            image = ((image - image.mean(axis=(0,1),dtype='float64')))/(image.std(axis=(0,1),dtype='float64'))\n",
    "            data[i] = image\n",
    "\n",
    "    if not train:\n",
    "        return data, None\n",
    "    \n",
    "    if operations['augment']:\n",
    "        data = PIL_Images_from_np_array(data)\n",
    "        if gts is not None:\n",
    "            gts = PIL_Images_from_np_array(gts)\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_groundtruths = []\n",
    "\n",
    "        for image in data:\n",
    "            augmented_imgs = []\n",
    "            augmented_imgs.append(image.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "\n",
    "            augmented_imgs.append(image.transpose(Image.ROTATE_90))\n",
    "\n",
    "            augmented_imgs.append(image.transpose(Image.ROTATE_180))\n",
    "\n",
    "            augmented_imgs.append(image.transpose(Image.ROTATE_270))\n",
    "\n",
    "            augmented_imgs.append(image.transpose(Image.FLIP_TOP_BOTTOM))\n",
    "\n",
    "            \n",
    "            if train:\n",
    "              augmented_imgs.append(image.filter(ImageFilter.GaussianBlur(4)))    \n",
    "              color_shift = ImageEnhance.Color(image)\n",
    "              augmented_imgs.append(color_shift.enhance(0.5))\n",
    "              augmented_imgs.append(image.rotate(10, resample=Image.BICUBIC))\n",
    "              augmented_imgs.append(image.rotate(20, resample=Image.BICUBIC))\n",
    "              augmented_imgs.append(image.rotate(30, resample=Image.BICUBIC))\n",
    "              augmented_imgs.append(image.rotate(40, resample=Image.BICUBIC))\n",
    "              augmented_imgs.append(image.rotate(50, resample=Image.BICUBIC))\n",
    "              augmented_imgs.append(image.rotate(60, resample=Image.BICUBIC))\n",
    " \n",
    "            augmented_images.extend(augmented_imgs)\n",
    "        if gts is not None:\n",
    "            for image in gts:\n",
    "                augmented_imgs = []\n",
    "                augmented_imgs.append(image.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "\n",
    "                augmented_imgs.append(image.transpose(Image.ROTATE_90))\n",
    "\n",
    "                augmented_imgs.append(image.transpose(Image.ROTATE_180))\n",
    "\n",
    "                augmented_imgs.append(image.transpose(Image.ROTATE_270))\n",
    "\n",
    "                augmented_imgs.append(image.transpose(Image.FLIP_TOP_BOTTOM))\n",
    "\n",
    "                if train:\n",
    "                  augmented_imgs.append(image)\n",
    "                  augmented_imgs.append(image)\n",
    "                  augmented_imgs.append(image.rotate(10, resample=Image.BICUBIC))\n",
    "                  augmented_imgs.append(image.rotate(20, resample=Image.BICUBIC))\n",
    "                  augmented_imgs.append(image.rotate(30, resample=Image.BICUBIC))\n",
    "                  augmented_imgs.append(image.rotate(40, resample=Image.BICUBIC))\n",
    "                  augmented_imgs.append(image.rotate(50, resample=Image.BICUBIC))\n",
    "                  augmented_imgs.append(image.rotate(60, resample=Image.BICUBIC))\n",
    "                \n",
    "                augmented_groundtruths.extend(augmented_imgs)\n",
    "        data.extend(augmented_images)\n",
    "        if gts is not None:\n",
    "            gts.extend(augmented_groundtruths)\n",
    "        data = np.array([np.array(image) for image in data])\n",
    "        if gts is not None:\n",
    "            gts = np.array([np.array(image) for image in gts])\n",
    "\n",
    "    if operations['patches'] and gts is not None:\n",
    "        patch_size = PATCH_SIZE\n",
    "        data = [crop(image,patch_size,patch_size) for image in data]\n",
    "        data = np.asarray([data[i][j] for i in range(len(data)) for j in range(len(data[i]))])\n",
    "        gts = [crop(image,patch_size,patch_size) for image in gts]\n",
    "        gts = np.asarray([gts[i][j] for i in range(len(gts)) for j in range(len(gts[i]))])\n",
    "        data = images_to_np_array(data)\n",
    "        gts = images_to_np_array(gts)\n",
    "\n",
    "    return data, gts\n",
    "        \n",
    "def crop(image, width, height):\n",
    "    res = []\n",
    "    for i in range(0,image.shape[1],height):\n",
    "        for j in range(0,image.shape[0],width):\n",
    "            if len(image.shape) == 2:\n",
    "                res.append(image[j:j + width, i : i + height])\n",
    "            else:\n",
    "                res.append(image[j:j + width, i : i + height, :])\n",
    "    return res\n",
    "\n",
    "def load_data(path_data, path_gts, train : bool, device, operations):\n",
    "    data = [Image.open(img) for img in path_data]\n",
    "    gts = []\n",
    "    if path_gts is not None:\n",
    "        gts = [Image.open(gt) for gt in path_gts]\n",
    "    \n",
    "    if train:\n",
    "        data, gts = preprocess(data, gts, operations,True)\n",
    "    else:\n",
    "        data, _ = preprocess(data, None, operations, False)\n",
    "\n",
    "    return data, gts\n",
    "\n",
    "def postprocess(y):\n",
    "    res = []\n",
    "    y = np.array(y)\n",
    "    sz = y[0][0].shape[0]\n",
    "    y = y.reshape((-1,6,sz,sz))\n",
    "    for images in y:\n",
    "        one = np.fliplr(images[1])\n",
    "        two = np.rot90(images[2], k=3)\n",
    "        three = np.rot90(images[3], k=2)\n",
    "        four = np.rot90(images[4],k=1)\n",
    "        five = np.flipud(images[5]) \n",
    "\n",
    "        m = np.stack([images[0], one, two, three, four, five])\n",
    "        m = np.mean(m,axis=0)\n",
    "        res.append(m)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationDataset(Dataset):\n",
    "    def __init__(self, data_path, gt_path, operations: dict, train: bool, device = None):\n",
    "        self.train = train\n",
    "        self.device = device\n",
    "        imgs, gts = load_data(data_path,gt_path,train,device,operations)\n",
    "        divide = not operations['normalization']\n",
    "        if gts is not None:\n",
    "            self.gt = [mask_to_tensor(gt,device) for gt in gts]\n",
    "        self.data = [image_to_tensor(img, device, divide = divide) for img in imgs] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.data[index], self.gt[index]\n",
    "        else:\n",
    "            return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadSegmentationModel(nn.Module):\n",
    "    def __init__(self, device, lr = LR, th =TH, max_iter = MAX_ITER):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.pre_trained_network = deeplab_model()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.pre_trained_network.to(self.device)\n",
    "        self.th = th\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def forward(self, data):\n",
    "        data_x = data[0].to(self.device)\n",
    "        return self.pre_trained_network(data_x)\n",
    "\n",
    "    def train_epoch(self, loader, optimizer):\n",
    "        loss = []\n",
    "        self.pre_trained_network.train()\n",
    "        for batch in tqdm(loader):\n",
    "            pr = self.forward(batch)\n",
    "            y = batch[1].to(self.device)\n",
    "            l = self.criterion(pr,y).to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l = l.cpu().detach().numpy()\n",
    "            loss.append(l)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def test_epoch(self, loader):\n",
    "        self.pre_trained_network.eval()\n",
    "        loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader):\n",
    "                pr = self.forward(batch)\n",
    "                y = batch[1].to(self.device)\n",
    "                l = self.criterion(pr,y).to(self.device)\n",
    "                l = l.cpu().detach().numpy()\n",
    "                loss.append(l)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def get_score(self,loader, do_postprocessing=True):\n",
    "        self.pre_trained_network.eval()\n",
    "        prs = []\n",
    "        ys = []\n",
    "        first_predictions = self.make_prediction(loader,do_postprocessing)\n",
    "        masks = loader.dataset.gt\n",
    "\n",
    "        for pr, y in zip(first_predictions, masks):\n",
    "            labels, _ = transform_prediction_to_patch(pr,1,th=self.th)\n",
    "            prs.extend(labels)\n",
    "            y = y[0].cpu().detach().numpy()\n",
    "            real_labels, _ = transform_prediction_to_patch(y,1,th=self.th)\n",
    "            ys.extend(real_labels)\n",
    "        return f1_score(ys,prs), accuracy_score(ys,prs)\n",
    "\n",
    "    def make_prediction(self, loader, do_postprocessing):\n",
    "        pr = []\n",
    "        self.pre_trained_network.eval()\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader):\n",
    "                p = sigmoid(self.pre_trained_network(batch.to(self.device))).cpu().detach().numpy()\n",
    "                pr.append(p[0][0])\n",
    "        if do_postprocessing:\n",
    "            pr = postprocess(pr)\n",
    "        return pr\n",
    "\n",
    "\n",
    "    def train(self, train_loader, test_loader, evaluate, evaluate_loader, do_postprocessing):\n",
    "        optimizer = torch.optim.Adam(self.pre_trained_network.parameters(), lr=self.lr)\n",
    "        losses = []\n",
    "        test_losses = []\n",
    "        accuracies = []\n",
    "        f1s = []\n",
    "        best_loss = {'loss': float('inf'), 'epoch': 0}\n",
    "        i = 1\n",
    "        while True:\n",
    "            print(\"EPOCH \" + str(i))\n",
    "            l_train = self.train_epoch(train_loader, optimizer)\n",
    "            print(\"epoch trained, now testing\")\n",
    "            losses.append(l_train)\n",
    "            l_test = self.test_epoch(test_loader)\n",
    "            test_losses.append(l_test)\n",
    "            print(\"Test Loss for epoch \" + str(i) + \" = \" + str(l_test))\n",
    "            f1 = 0\n",
    "            acc = 0\n",
    "            if evaluate:\n",
    "                f1, acc = self.get_score(evaluate_loader, do_postprocessing)\n",
    "                print(\"LOSS = \" + str(l_test) + \" F1 = \" + str(f1) + \" ACCURACY = \" + str(acc))\n",
    "\n",
    "            f1s.append(f1)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            if l_test < best_loss['loss']:\n",
    "                best_loss['loss'] = l_test\n",
    "                best_loss['epoch'] = i\n",
    "\n",
    "            if i == self.max_iter - 1:\n",
    "                break\n",
    "\n",
    "            i += 1\n",
    "        results = {}\n",
    "        results['train_loss'] = losses\n",
    "        results['f1'] = f1s\n",
    "        results['accuracy'] = accuracies\n",
    "        results['test_loss'] = test_losses\n",
    "        return results\n",
    "\n",
    "    def submit(self, test_loader):\n",
    "        \n",
    "        prs = self.make_prediction(test_loader,False)\n",
    "        img_ids = range(1,len(prs)+1)\n",
    "        \n",
    "        ret_ids = []\n",
    "        ret_labels = []\n",
    "\n",
    "        for pr, i in zip(prs,img_ids):\n",
    "            labels, ids = transform_prediction_to_patch(pr,i,th=self.th)\n",
    "            for label in labels:\n",
    "                ret_labels.append(label)\n",
    "            for id in ids:\n",
    "                ret_ids.append(id)\n",
    "        \n",
    "        pd.DataFrame({'id': ret_ids, 'prediction' : ret_labels}).to_csv(SUBMISSION_PATH,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do Cross-Validation oon TH and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(N, k_fold, seed = SEED):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        N:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = N\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation_step(data, gts, k_indices, k, th, lr):\n",
    "    train_data = np.delete(data, k_indices[k], axis = 0)\n",
    "    train_gts = np.delete(gts, k_indices[k], axis = 0)\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for id in k_indices[k]:\n",
    "        test_x.append(data[id])\n",
    "        test_y.append(gts[id])\n",
    "    operations = {}\n",
    "    operations['augment'] = True\n",
    "    operations['normalization'] = True\n",
    "    operations['patches'] = True\n",
    "\n",
    "    \n",
    "    train_set = RoadSegmentationDataset(train_data,train_gts,operations, True, DEVICE)\n",
    "    test_set = RoadSegmentationDataset(test_x,test_y,operations,True,DEVICE)\n",
    "    evaluation_dataset = RoadSegmentationDataset(test_x,test_y,operations,False,DEVICE)\n",
    "\n",
    "    train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_set,BATCH_SIZE, shuffle=False)\n",
    "    evaluation_loader = DataLoader(evaluation_dataset,1, shuffle=False)\n",
    "\n",
    "    model = RoadSegmentationModel(DEVICE,th=th,lr=lr,max_iter=10)\n",
    "    results = model.train(train_loader, test_loader,True,evaluation_loader,False)\n",
    "    f1_score = results[\"f1\"]\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def validation_over_threshold(data, gts, k_indices, k, threshold):\n",
    "    return cross_validation_step(data,gts,k_indices,k,threshold,LR)\n",
    "\n",
    "def validation_over_learning_rate(data, gts, k_indices, k, lr):\n",
    "    return cross_validation_step(data,gts,k_indices,k,TH,lr)\n",
    "\n",
    "def cross_validation(data, gts, parameters, parameter_name, N, seed = SEED, k_fold = K_FOLD):\n",
    "    k_indices = build_k_indices(N,k_fold,seed)\n",
    "    best_performance = -1\n",
    "    optimal_parameter = -1\n",
    "\n",
    "    for parameter in parameters:\n",
    "        print(\"Trying \" + str(parameter_name) + \" = \" + str(parameter))\n",
    "        avg_performance = 0\n",
    "        performances = np.zeros(k_fold)\n",
    "\n",
    "        for k in range(k_fold):\n",
    "            performance = 0\n",
    "            if parameter_name == \"threshold\":\n",
    "                performance = validation_over_threshold(data,gts,k_indices,k,parameter)[-1]\n",
    "            elif parameter_name == \"learning rate\":\n",
    "                performance = validation_over_learning_rate(data,gts,k_indices,k,parameter)[-1]\n",
    "            avg_performance = performance + avg_performance\n",
    "            performances[k] = performance\n",
    "        \n",
    "        avg_performance = avg_performance / k_fold\n",
    "\n",
    "        print(\"Cross-Validation for \" + parameter_name + \" = \" + str(parameter) + \" with f1_score = \" + str(avg_performance))\n",
    "        if best_performance == -1 or avg_performance > best_performance:\n",
    "            best_performance = avg_performance\n",
    "            optimal_parameter = parameter\n",
    "\n",
    "    print(\"Optimal Patameter for \" + parameter_name + \" = \" + str(optimal_parameter))\n",
    "    return optimal_parameter, best_performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which experiment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 4\n",
    "\n",
    "if EXPERIMENT == 1:\n",
    "    divide_patches = True\n",
    "    normalize = False\n",
    "    augment = False\n",
    "    training = True\n",
    "    submmission = True\n",
    "elif EXPERIMENT == 2:\n",
    "    divide_patches = True\n",
    "    normalize = True\n",
    "    augment = False\n",
    "    training = True\n",
    "    submmission = True\n",
    "elif EXPERIMENT == 3:\n",
    "    divide_patches = True\n",
    "    normalize = False\n",
    "    augment = True\n",
    "    training = True\n",
    "    submmission = True\n",
    "elif EXPERIMENT == 4:\n",
    "    divide_patches = True\n",
    "    normalize = False\n",
    "    augment = True\n",
    "    training = True\n",
    "    submmission = True\n",
    "    TH = 0.25\n",
    "    LR = 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training and produce submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    operations = {}\n",
    "    operations['augment'] = augment\n",
    "    operations['normalization'] = normalize\n",
    "    operations['patches'] = divide_patches\n",
    "\n",
    "    data = sorted(glob(DATA_PATH + \"/training/images/*.png\"))\n",
    "    gts = sorted(glob(DATA_PATH + \"/training/groundtruth/*.png\"))\n",
    "\n",
    "    optimal_th = TH\n",
    "    optimal_lr = LR\n",
    "    if validation:\n",
    "        print(\"Starting Cross Validation over Foreground Threshold\")\n",
    "        optimal_th,_ = cross_validation(data,gts,THRESHOLD_VALIDATION_VECTOR,\"threshold\",len(data))\n",
    "        print(\"Starting Cross Validation over Learning Rate\")\n",
    "        optimal_lr,_ = cross_validation(data,gts,LEARNING_RATE_VALIDATION_VECTOR,\"learning rate\",len(data))\n",
    "    \n",
    "    train_data, train_labels, test_data, test_labels = split_data(data,gts)\n",
    "\n",
    "    if submmission:\n",
    "        # run training on full dataset\n",
    "        train_data = data\n",
    "        train_labels = gts\n",
    "    train_set = RoadSegmentationDataset(train_data,train_labels,operations, True, DEVICE)\n",
    "    test_set = RoadSegmentationDataset(test_data,test_labels,operations,True,DEVICE)\n",
    "\n",
    "    train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_set,BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RoadSegmentationModel(DEVICE, th= optimal_th, lr = optimal_lr)\n",
    "\n",
    "    if training:\n",
    "        results = model.train(train_loader, test_loader,False,None,augment)\n",
    "        loss = results['train_loss']\n",
    "        test_loss = results['test_loss']\n",
    "        print(\"TRAINING LOSS = \" + str(loss[len(loss)-1]))\n",
    "        print(\"TEST LOSS = \" + str(test_loss[len(test_loss)-1]))\n",
    "    if submmission:\n",
    "        submission_images = sorted(glob(DATA_PATH + \"/test_set_images/*/*\"),\n",
    "            key = lambda x: int(x.split('/')[-2].split('_')[-1]))\n",
    "\n",
    "        test_set = RoadSegmentationDataset(submission_images, None, operations, False, DEVICE)\n",
    "        test_loader = DataLoader(test_set,1)\n",
    "\n",
    "        model.submit(test_loader)\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d1bc666db819c5d4531cfbb2a45e55ecf4b175539248574842460daf230290"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
